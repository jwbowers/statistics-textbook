---
title: "On Teaching Statistics: Truth, Power, and the Theory in Our Hands"
author: "Jake Bowers"
---

# Why This Book Exists (And Why It's Different)

This is not a typical statistics textbook. You will not find, in these pages, a march through theorems followed by worked examples. You will not be asked to passively absorb a sequence of techniques and then reproduce them on homework problems. Instead, this book invites you to *discover* statistical ideas through exploration, to develop statistical intuitions through simulation and computation, and to build the capability to use statistics as what it truly is: a powerful language for engaging with evidence, for making and evaluating claims about the world, and for participating in democratic and scientific discourse.

This pedagogical approach is grounded in a simple but radical principle: **a teacher should never lie, and a teacher's primary responsibility is to empower students to develop their own capabilities**. This principle has profound implications for how we teach statistics.

# The Language of Power

When we teach statistics, we are not simply teaching a set of mathematical techniques. We are teaching what Lisa Delpit [-@delpit1988power; -@delpit2006other] calls a "language of power"—a discourse that structures how evidence is gathered, evaluated, and deployed in policy debates, scientific arguments, and public deliberation. Those who command this language can participate fully in these conversations. Those who do not are excluded, or worse, become subjects of analysis rather than agents of inquiry.

Traditional statistics pedagogy often mystifies this language, presenting it as the exclusive domain of mathematical sophistication, accessible only to those with extensive calculus training. But this gatekeeping is unnecessary and unjust. Modern computing has democratized statistical practice: simulation can replace derivation, computational exploration can build intuition that once required years of mathematical training, and visualization can make abstract concepts concrete.

To withhold this access, to insist that students must first master mathematical prerequisites that have become optional in actual statistical practice, is to perpetuate exclusion. It is also, in a real sense, a form of lying—suggesting that statistical thinking requires capabilities that it no longer requires, and hiding the fact that the language of power is more accessible than students have been led to believe.

# Truth-Telling in Statistical Education

What does it mean to refuse to lie when teaching statistics? It means several things:

**First, it means acknowledging uncertainty honestly.** Statistics is fundamentally about reasoning under uncertainty, yet too many textbooks present statistical procedures as mechanical recipes that produce "the answer." This book takes seriously the reality that every statistical analysis involves choices, assumptions, and judgments. We explore these openly rather than hiding them behind technical jargon or mathematical notation.

**Second, it means being honest about what statistics can and cannot do.** Statistical analysis cannot establish causal relationships from observational data without strong assumptions. Hypothesis tests do not tell us whether our scientific theories are true. Regression coefficients do not necessarily have causal interpretations. We will confront these limitations directly, because recognizing what a tool cannot do is essential to using it wisely.

**Third, it means transparency about the political and ethical dimensions of statistical practice.** Every choice in a statistical analysis—what to measure, who to include, which comparisons to make—reflects values and has consequences. These are not purely technical decisions, and we will not pretend that they are.

This commitment to truth-telling extends to the structure of this book. Unlike traditional textbooks that present a sanitized version of statistical practice, we include false starts, exploration of ideas that don't work, and genuine questions to which we don't have complete answers. This is how statistical thinking actually proceeds, and students deserve to see it honestly.

# Discovery Before Formalization: Problem-Posing Pedagogy

The educational philosophers who inspire this book—Paulo Freire [-@freire1970pedagogy], Maria Montessori [-@montessori1967absorbent], and John Dewey [-@dewey1938experience]—all understood that learning requires active engagement, not passive reception. Freire particularly emphasized the distinction between "banking education" (where teachers deposit knowledge into students' heads) and "problem-posing education" (where teachers and students engage together in understanding the world).

In this book's approach:

- **You will explore first, formalize later.** Each topic begins with a question or puzzle, not a definition. You will generate data, make comparisons, observe patterns, and develop intuitions before encountering formal statistical frameworks.

- **You will learn through failure.** Statistical thinking develops through confronting difficult problems, making mistakes, and understanding why certain approaches don't work. The "explorations" in this book are designed to challenge you, sometimes to create productive confusion that motivates the need for more sophisticated tools.

- **You will direct your own learning.** The book provides questions and provocations, but you will often need to determine how to answer them. This uncertainty is intentional. In your future work, no textbook will provide step-by-step instructions for your specific problem.

This approach has roots in the work of other educational thinkers as well:

- **Jerome Bruner**'s [-@bruner1960process] theory of discovery learning emphasizes that students construct understanding through active exploration rather than passive receipt of information.

- **Seymour Papert**'s [-@papert1980mindstorms] constructionism argues that learning happens most powerfully when students build things—in our case, when they write code, generate simulations, and construct arguments from data.

- **Lev Vygotsky**'s [-@vygotsky1978mind] concept of the zone of proximal development reminds us that learning happens at the edge of current capability, where tasks are challenging but achievable with effort and support.

# Theory in the Hands: Statistical Thinking as Skilled Practice

In his remarkable study *Ways of the Hand* [-@sudnow2001ways], David Sudnow describes learning to play jazz piano not as a matter of consciously applying music theory, but as developing embodied knowledge—theory that exists "in the hands" rather than (or in addition to) "in the head." A skilled jazz pianist doesn't pause to calculate which notes form a minor seventh chord; their fingers know where to go.

Statistical thinking, I contend, works similarly. Yes, there is important theory—ideas about bias and variance, about conditional probability, about causal identification. But to use statistics creatively and effectively, this theory must become embodied knowledge, the kind of understanding that shapes intuition and guides action without requiring conscious retrieval of formal definitions.

How does theory get "into the hands"? Through practice—lots of it, and in varied contexts. Through repeated engagement with data, through writing and revising code, through making predictions about what an analysis will show and discovering whether those predictions hold. Through developing the pattern recognition that lets you look at a scatter plot and immediately sense heteroskedasticity, or examine a research design and recognize a potential confound.

This is why this book emphasizes computation and simulation so heavily. When you simulate a sampling distribution, you are not just verifying a mathematical theorem (though you are doing that). You are developing intuition about variability, about the relationship between sample and population, about the meaning of a standard error. When you write code to implement a randomization test, the theory of permutation inference enters your hands.

This approach has important implications. It means:

- **You will write a lot of code.** Statistical computing is not an add-on to statistical thinking; it is increasingly the primary way statistical thinking happens in practice.

- **You will work with real data, with all its messiness.** Textbook examples with clean, well-behaved data do not prepare you for actual statistical practice.

- **You will iterate and revise.** Your first attempt at an analysis will rarely be your best. Learning to examine, critique, and improve your own work is essential.

# The Revolution in Statistics Education: Computation Changes Everything

The pedagogical approach of this book is not idiosyncratic. It is part of a broader transformation in how statistics is taught, led by statisticians and educators who recognized that computers have fundamentally changed what is pedagogically optimal.

## Cobb's Ptolemaic Curriculum

In a justly influential 2007 article, George Cobb [-@cobb2007introductory] posed a provocative question: Is the introductory statistics course "a Ptolemaic curriculum"? His argument was that the standard statistics curriculum, despite many improvements, remains "an unwitting prisoner of history."

The metaphor is powerful. Ptolemy's geocentric model of astronomy worked—it could predict planetary positions. But it required increasingly complex corrections ("epicycles") to account for observations. The system was complicated not because reality was complicated, but because the foundational model was wrong. When Copernicus proposed that the Earth orbits the Sun, not vice versa, astronomy became conceptually simpler and more elegant.

Cobb argued that traditional statistics pedagogy faces the same problem. We teach "the technical machinery of numerical approximations based on the normal distribution"—the *t*-distribution, pooled variance estimates, corrections for unequal variances, approximations for degrees of freedom, and on and on. Each new situation requires another epicycle, "another adjustment [that] takes students' attention away from more basic ideas such as the fit between model and reality" [@cobb2007introductory].

But this machinery "was once necessary because the conceptually simpler alternative based on permutations was computationally beyond our reach." Before computers, statisticians had no choice but to develop mathematical approximations that could be calculated by hand or looked up in tables. The normal distribution machinery wasn't chosen because it was pedagogically optimal or conceptually clearest—it was chosen because it was computationally feasible.

**Now that we have computers, we can teach statistics in the order that makes conceptual sense**, not the order dictated by what humans can calculate without machines. Permutation tests and randomization-based inference are conceptually more direct—they "make a direct connection between data production and the logic of inference that deserves to be at the core of every introductory course" [@cobb2007introductory]. Simulation-based approaches let students see sampling distributions rather than imagine them, understand the bootstrap rather than memorize formulas for standard errors.

This is not "dumbing down" statistics—it is teaching the logic clearly rather than obscuring it with historical artifacts. To continue teaching the Ptolemaic curriculum when a Copernican alternative exists is, in a real sense, a form of lying: suggesting that the complex machinery is necessary when it is not, and hiding the more direct logical path because of computational constraints that no longer exist.

## Kaplan's Modeling-First Approach

Daniel Kaplan, in his textbook *Statistical Modeling: A Fresh Approach* [-@kaplan2012ism] and subsequent work, developed a coherent alternative curriculum organized around three key insights:

**First, start with models, not with descriptive statistics.** Traditional courses begin with means and standard deviations, gradually build to simple linear regression, and only near the end (if at all) consider models with multiple predictors. But this sequence is historically contingent, not pedagogically optimal. Real applications almost always involve multiple variables. By starting with modeling from day one, students immediately engage with the complexity of actual data analysis rather than spending weeks on oversimplified cases that must later be unlearned.

**Second, organize inference around "Randomize, Repeat, Reject."** This algorithmic approach—randomize the assignment or resample the data, repeat many times to build a reference distribution, reject the null hypothesis if your observed result is extreme—is conceptually more accessible than the traditional route through probability theory, sampling distributions, and the Central Limit Theorem. Kaplan notes that this approach, building on Cobb's work, lets students understand the *logic* of inference before getting bogged down in mathematical machinery.

**Third, embrace computation as the primary mode of statistical work.** Kaplan famously stated: "The purpose of computing is insight, not numbers." Computation is not a concession to mathematically weaker students; it is how modern statistics actually works. By teaching R from the beginning (Kaplan was among the first to teach introductory statistics entirely with R, starting in 1997), students learn to use professional tools, making them more capable, not less, than students who learn to manipulate formulas by hand.

These principles directly inform this textbook. You will work with models and multiple variables from early on. You will learn inference through simulation and randomization before (and sometimes instead of) mathematical approximations. You will use R not as an afterthought but as your primary tool for statistical thinking.

## The MOSAIC Project and Educational Reform

Kaplan's work is part of the broader Project MOSAIC (Models, Statistics, and Computation), a collaborative effort to modernize statistics education. The project explicitly recognizes that computation changes what is pedagogically possible and optimal. The `mosaic` package for R that you will use in this book emerged from this effort to provide tools that make sophisticated analysis accessible and readable.

This educational reform movement aligns with the principles articulated earlier: truth-telling (teach what is conceptually clearest, not what is computationally convenient with 1950s technology), empowerment (give students the actual tools used in professional practice), and discovery-based learning (let students explore through simulation rather than memorize formulas).

The choice to teach statistics computationally is thus both pedagogical and ethical. It is pedagogical because simulation builds intuition more effectively than derivation for most students. It is ethical because it democratizes access to statistical thinking, removing artificial barriers while preserving—indeed enhancing—intellectual rigor.

# Scaffolding and Support

Emphasizing discovery and autonomy does not mean abandoning students to struggle alone. Lev Vygotsky's concept of "scaffolding" is crucial: learning happens most effectively when students work at the edge of their current capabilities, supported by tools, structure, and guidance that enable them to accomplish things they could not yet do independently.

This book provides scaffolding in several ways:

- **Structured explorations** that guide inquiry while leaving room for your own questions and directions
- **Worked examples** that demonstrate not just techniques but the *process* of statistical reasoning
- **Simulation frameworks** that make complex ideas concrete and manipulable
- **Prompts for reflection** that encourage you to articulate and examine your developing understanding

The goal is for these scaffolds to become progressively less necessary as your capabilities develop, as you internalize the patterns of statistical thinking and make them your own.

# Who This Book Is For

This book is for anyone who wants to use statistics to engage with evidence and make arguments about the world. You might be:

- A graduate student preparing to conduct empirical research
- An undergraduate learning to read and evaluate quantitative claims
- A policy analyst who needs to understand and critique evidence
- A data scientist wanting to deepen your understanding of statistical foundations
- Anyone curious about how we can learn from data

You do *not* need extensive mathematical preparation. You *do* need:

- Willingness to engage seriously with challenging ideas
- Comfort (or willingness to become comfortable) with basic programming
- Intellectual curiosity and persistence
- Openness to uncertainty and to changing your mind

# A Different Kind of Journey

bell hooks [-@hooks1994teaching] writes about "engaged pedagogy"—teaching that is about freedom, that takes seriously the intellectual and ethical development of students as whole people, that refuses to treat education as neutral transmission of information. This book aspires to that vision.

You are not here to memorize formulas or to become a calculation machine. You are here to develop capabilities: to think carefully about evidence, to make and evaluate arguments, to use statistical ideas creatively in service of understanding the world and acting in it.

The journey will sometimes be uncomfortable. Discovery-based learning means spending time confused, trying approaches that don't work, confronting the limits of your current understanding. But this discomfort is productive. As Dewey understood, genuine learning—learning that transforms understanding rather than merely adding to it—requires disequilibrium and reconstruction.

You will surprise yourself with what you can do. Students often underestimate their own capabilities, especially in technical domains. One of my greatest joys as a teacher is watching students discover that they can accomplish things they didn't think possible—that they can write sophisticated code, that they can design and execute complex analyses, that they can make original contributions to knowledge. This book is designed to create opportunities for those discoveries.

# On Perfection and Growth

A final word on evaluation and growth. The purpose of the work you do with this book is *not* to demonstrate that you already know everything. It is to *develop new capabilities*. This means:

- **Mistakes are necessary and valuable.** If you're not making mistakes, you're not working at the edge of your current understanding, which means you're not learning efficiently.

- **Improvement matters more than initial performance.** Your second attempt at an analysis should be better than your first. Your understanding at the end of the semester should be transformed from where you began.

- **Asking for help is a strength, not a weakness.** Knowing when you're stuck and need assistance, and being able to articulate what you need, are crucial professional skills.

- **Perfect is the enemy of good (and of learning).** Your goal is not to produce flawless analyses but to develop understanding and capability. Sometimes a rough simulation that builds intuition is more valuable than an elegant proof.

# The Structure of This Book

This book is organized around major themes in statistical thinking:

**Part I: Description** explores how we summarize and communicate about data. Before we can make inferences or establish causation, we must be able to describe clearly what we observe.

**Part II: Statistical Inference** develops frameworks for reasoning from samples to populations, for quantifying uncertainty, and for testing hypotheses.

**Part III: Causal Inference** confronts the most challenging question in empirical research: how do we establish what causes what?

**Part IV: Adjustment** explores strategies for addressing confounding through various forms of statistical and design-based control.

**Part V: Large Sample Theory** examines the mathematical foundations that justify many common statistical practices, but through simulation and exploration rather than formal proof.

**Part VI: Model-Based Inference** introduces frameworks for statistical modeling, while maintaining appropriate skepticism about what models can accomplish.

Each part is designed to be explored actively. You will write code, generate data, test hypotheses (both statistical and conceptual), and engage with real problems and datasets.

# An Invitation

This book invites you into a conversation—with ideas, with data, with other learners, and with the broader community of people trying to understand the world through systematic inquiry. It is a conversation that will continue long after you finish this book, as you encounter new methods, new problems, and new challenges.

Welcome to this journey. Let's begin to discover statistical thinking together.

# References

::: {#refs}
:::
